---
title: "Simulation plot"
author: "Yichen"
date: "6/2/2020"
output: html_document
---

```{r setup, include=FALSE}
options(width = 10000)
knitr::opts_chunk$set(eval = TRUE, warning = FALSE)

library(ggplot2)
library(dplyr)
library(gridExtra)
rm(list = ls())
setwd("/Users/Yichen/Google Drive/Genentech/simulation/060220/res2")
list.filenames<-list.files(pattern=".csv$")
list <- lapply(1: length(list.filenames), function(i) {
  read.csv(list.filenames[i])
})
d <- do.call(rbind, list)

beta.check <- d %>% filter(grepl("beta", r2.type, ignore.case = TRUE), cor.random == 0)


#Graphing
thm<-theme(plot.title = element_text(hjust = 0.5, size=14,face="bold"), 
           plot.subtitle = element_text(hjust = 0.5, size=12),
           legend.position = "none",
           # legend.title = element_text(size=14),
           # legend.text = element_text(size=12),
           plot.caption = element_text(hjust = 0, face= "italic"),
           plot.caption.position = "plot",
           axis.title=element_text(size=14),
           axis.text=element_text(size=12))
```

# {.tabset .tabset-pills .tabset-fade}
## Simulation 1 {.tabset .tabset-pills .tabset-fade}
### Parameters {.tabset .tabset-pills .tabset-fade}
* **Parameter**
  * 3 legacy trials and 1 future trial (n = 600 each)
  *	For each trial:
    * 1 causal feature x (not observed) with coefficient \beta
    * Outcome $y = \beta x + \epsilon$, $x$ not observed in data
    *	$Q$ candidate correlated features
    *	$q$ truly correlated features with correlation $\rho$ (randomly selected)
    * 15 noise features

* **Outcomes**
  * Oracle $R^2$ (orange solid line)
  * Generalized $R^2$ (blue solid line)
  * Estimated generalized $R^2$ with 10-fold cross-validation (blue dashed line)
  * Estimated generalized $R^2$ with leave-one-study-out cross-validation (blue dotted line)

* **Detailed explanation**
  * <span style="color:blue">Oracle $R^2$ is calculated analytically
      * assume we do not observe the causal feature x but we know $\beta$, $\rho$ and which features are correlated over the 3 legacy trials
      * suppose we are given a new (not previously seen) observation from one of the 3 legacy trials and we **know** which trial the new observation comes from
      * we then build the best simple linear regression model for predicting the y value and the oracle $R^2$ is derived from this model</span>

  * Generalized $R^2$ is calculated empirically using the model built on legacy data to predict observations from future trial $$R^2 = 1 - \frac{MSPE}{MSE} = 1 - \frac{\frac{1}{n}\sum(y - \hat y)^2}{\frac{1}{n}\sum(y - \overline y)^2}$$

  * Estimated generalized $R^2$ from 10-fold cross-validation is obtained as follows
    * Combine the data from all 3 legacy trials
    * Train the model on 9 folds of the data and predict on the left-over 1 fold
    * Repeat the process for each of the 10 fold and average the 10 $R^2$
  
  * Estimated generalized $R^2$ from leave-one-out cross-validation is obtained as follows
    * Train the model on 2 of the legacy trials and predict on the other trial to obtain MSPE. 
    * Repeat the process for each of the 3 legacy trials and average the 3 $R^2$


### Result 1a {.tabset .tabset-pills .tabset-fade}

- **Hypothesis: As number of truly correlated features $q$ increases, the correlated features picked up by the legacy model are more likely to overlap with correlated features in the future trials, which leads to smaller difference between generalized $R^2$ and estimated generalized $\hat R^2$**
- Fix Q, as q increase, generalized $R^2$ and two $\hat R^2$ all increase towards the oracle $R^2$. The difference between generalized $R^2$ and  $\hat R^2$ decreases as we have a larger proportion of candidate correlated features among candidate correlated features.
- While $\hat R^2$ from 10-fold cv largely overestimates the generalized $R^2$, $\hat R^2$ from loo cv corrects this gap and does a better job at estimating the generalized $R^2$.
- <span style="color:blue">Lasso regression performs similarly compared to simple linear regression when we have small number of correlated features </span>

```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    !grepl("unknown", r2.type, ignore.case = TRUE),
                    cor.random == 1, n.cor == 15, cor == 0.9, beta == 1)
tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~use.method) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial unknown)"), color = "green") + labs(title= "Estimating generalized R-squared: increasing number of truly correlated features (q)", subtitle = "(Number of candidate correlated features Q = 15, \u03C1 = 0.9, \u03B2 = 1)", y="R-squard", x = "Number of truly correlated features q", linetype = "Type") + thm
```


- <span style="color:blue">Lasso regression outperforms simple linear regression when we have large number of correlated features </span>
```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    !grepl("unknown", r2.type, ignore.case = TRUE),
                    cor.random == 1, n.cor == 250, cor == 0.9, beta == 1)

tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~use.method) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial unknown)"), color = "green") + labs(title= "Estimating generalized R-squared: increasing number of truly correlated features (q)", subtitle = "(Number of candidate correlated features Q = 250, \u03C1 = 0.9, \u03B2 = 1)", y="R-squard", x = "Number of truly correlated features q", linetype = "Type") + thm
```

- With larger coefficient $\beta$, we will also have larger oracle $R^2$
- We observe larger difference between generalized $R^2$ and $\hat R^2$ from 10-fold cv.
- $\hat R^2$ from loo cv still does a good job estimating generalized $R^2$.
```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    !grepl("unknown", r2.type, ignore.case = TRUE),
                    cor.random == 1, n.cor == 15, cor == 0.9, use.method == "Lasso regression")
tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))

beta.labs = c("\u03B2 = 1", "\u03B2 = 10")
names(beta.labs) <- c("1", "10")
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~beta, labeller = labeller(beta = beta.labs), ncol = 2) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") +
  labs(title= "Estimating generalized R-squared with different coefficient using Lasso regression", subtitle = "(15 candidate correlated features, \u03C1 = 0.9)", y="R-squard", x = "Number of truly correlated features q") + thm
```

- With larger correlation $\rho$, we will also have larger oracle $R^2$. 
- While the $\hat R^2$ from 10-fold cv largely overestimate the generalized $R^2$, $\hat R^2$ from loo cv approximates generalized $R^2$ well, only mildly influenced by correlation change.
```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    !grepl("unknown", r2.type, ignore.case = TRUE),
                    cor.random == 1, n.cor == 15, beta == 1, use.method == "Lasso regression")
tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
cor.labs = c("\u03C1 = 0.5", "\u03C1 = 0.7", "\u03C1 = 0.9")
names(cor.labs) <- c("0.5", "0.7", "0.9")
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~cor, labeller = labeller(cor = cor.labs), ncol = 3) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") +
  labs(title= "Estimating generalized R-squared with different correlation using Lasso regression", subtitle = "(15 candidate correlated features, \u03B2 = 1)", y="R-squard", x = "Number of truly correlated features q", linetype = "Type") + thm
```



### Result 1b {.tabset .tabset-pills .tabset-fade}

- **Hypothesis: As candidate correlated features $Q$ increases, the correlated features in the legacy model are less likely to overlap with correlated features in the future trials. This leads to larger difference in generalized $R^2$ and $\hat R^2$.**
- Fix number of truly correlated feautures $q = 15$, as Q increase, both $R^2$ and $\hat R^2$ from 10-fold cv or loo cv decrease. But generalized $\hat R^2$ decreases much faster. We observe a larger difference between $R^2$ and $\hat R^2$ as the proportion of truly correlated features among candidate correlated features decreases. 
- Similarly, $\hat R^2$ from 10-fold cv overestimates the generalized $R^2$, $\hat R^2$ from loo cv performs better at estimating the generalized $R^2$.
- <span style="color:blue">Lasso regression performs similarly compared to simple linear regression when we only have 15 truly correlated features </span>

```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    !grepl("unknown", r2.type, ignore.case = TRUE),
                    cor.random == 1, n.cor.1 == 15, cor == 0.9, beta == 1)

tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
ggplot(tmp, aes(x = n.cor, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~use.method) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") +
  labs(title= "Estimating generalized R-squared: increasing number of candidate correlated features Q", subtitle = "(15 truly correlated features, \u03C1 = 0.9, \u03B2 = 1)", y="R-squard", x = "Number of truly correlated features q", linetype = "Type") + thm
```


## Simulation 2 {.tabset .tabset-pills .tabset-fade}

* Updated simulation
  * Distinct correlated features in each trial (no overlap)
  * <span style="color:blue">An additional oracle $R^2$ is calculated analytically (green solid line)
    * assume we do not observe the causal feature $x$ but we know $\beta$, $\rho$ and which features are correlated over the 3 legacy trials. 
    * the correlated features in each legacy trial will be distinct and there's no overlap.
    * suppose we are given a new (not previously seen) observation from one of the 3 legacy trials but we **do not know** which trial the new observation comes from
    * assume there is an equal probability that the new observation comes from each of the 3 trials
    * we then build the best simple linear regression model for predicting the y value , and the oracle $R^2$ is derived from this model.</span>


- <span style="color:blue">Lasso regression returns higher generalized $R^2$ and $\hat R^2$ overall. $\hat R^2$ from loo cv approximates generalized $R^2$ slightly better compared to that of simple linear regression.</span>
```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    cor.random == 0, n.cor == 200, cor == 0.9, beta == 1)

tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
cor.labs = c("\u03C1 = 0.5", "\u03C1 = 0.7", "\u03C1 = 0.9")
names(cor.labs) <- c("0.5", "0.7", "0.9")

ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~use.method) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial unknown)"), color = "green") + labs(title= "Estimating generalized R-squared with non-overlapping correlated features", subtitle = "(\u03C1 = 0.9, \u03B2 = 1)", y="R-squard", x = "Number of correlated features in each trial", linetype = "Type") + thm
```


- <span style="color:blue">With stronger correlation $\rho$, we observe larger generalized $R^2$ and $\hat R^2$. Oracle $R^2$ based on both assumptions increase with increasing number of truly correlated features and coefficient. 
- Overall, lasso regression returns higher generalized $R^2$ and $\hat R^2$ compared to simple linear regression.</span>
```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    cor.random == 0, n.cor.1 > 15, n.cor == 200, beta == 1, use.method == "Lasso regression")

tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~cor, labeller = labeller(cor = cor.labs),  ncol = 3) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial unknown)"), color = "green") + labs(title= "Estimating generalized R-squared with different correlation using Lasso regression", subtitle = "(\u03B2 = 1)", y="R-squard", x = "Number of correlated features in each trial", linetype = "Type") + thm
```

```{r, echo=FALSE, fig.width = 12, fig.height = 4}
tmp <- d %>% filter(!grepl("beta", r2.type, ignore.case = TRUE),
                    !grepl("legacy", r2.type, ignore.case = TRUE),
                    cor.random == 0, n.cor.1 > 15, n.cor == 200, beta == 1, use.method == "Simple linear regression")

tmp$r2.type <- factor(tmp$r2.type, levels = c("Generalized R-squared", "Estimated generalized R-squared w/ 10-fold cv", "Estimated generalized R-squared w/ loo cv", "Oracle R-squared (trial known)", "Oracle R-squared (trial unknown)"))
ggplot(tmp, aes(x = n.cor.1, y = r2, linetype = r2.type)) + geom_line() + facet_wrap(~cor, labeller = labeller(cor = cor.labs),  ncol = 3) + 
  scale_linetype_manual(values=c("solid", "dashed", "dotted", "solid", "solid")) + 
  geom_line(data = filter(tmp, !grepl("oracle", r2.type, ignore.case = TRUE)), color = "steelblue") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial known)"), color = "coral") + 
  geom_line(data = filter(tmp, r2.type == "Oracle R-squared (trial unknown)"), color = "green") + labs(title= "Estimating generalized R-squared with different correlation using Simple linear regression", subtitle = "(\u03B2 = 1)", y="R-squard", x = "Number of correlated features in each trial", linetype = "Type") + thm
```

## Calculation {.tabset .tabset-pills .tabset-fade}
### Observed causal feature {.tabset .tabset-pills .tabset-fade}
Oracle $R^2$ when we get to observe the causal feature $X$ is calculated as follows

$\begin{aligned}
y = \beta X + \epsilon, X \sim N(0, 1), \epsilon \sim N(0, 1) \\
y|X = x \sim N(\beta x, 1)\\

var(Y) 
&= var(\beta X + \epsilon) \\
&= \beta^2 var(X) + var(\epsilon) + 2Cov(X, \epsilon) \\
&= \beta^2 + 1 \\
\\
cov(y, X)  
&= E[yX] - E[y]E[X] \\
&= E[\beta X^2 + X\epsilon] - 0 \\
&= \beta E[X^2] + E[X]E[\epsilon] \\
&= \beta \\
\\
R^2 = 1 - \frac{var(y|x)}{var(y)} = 1 - \frac{var(\epsilon)}{\beta ^ 2var(x) + var(\epsilon)} = 1 - \frac{1}{\beta^2+1}\\

\end{aligned}$

### Causal feature not observed and trial number available {.tabset .tabset-pills .tabset-fade}

**Scenario 1**: We only have 1 legacy trial with 1 correlated feature $\tilde{X}$ such that $\tilde{X}|X = x = \rho x + \sqrt{1-\rho^2} Z$

Oracle $R^2$ is calculated as follows:

$\begin{aligned}
y = \tilde{\beta} \tilde{X} + \tilde{\epsilon}, \tilde{X} \sim N(0, 1) \\
cov(\tilde{X}, X) = \rho Var(X) Var(\tilde{X}) = \rho \\
\\
cov(y, \tilde{X}) 
&= E[y\tilde{X}] - E[y]E[\tilde{X}] \\
&= E[(\beta X + \epsilon) \tilde{X}] - 0 \\
&= E[(\beta X \tilde{X}] + E[\epsilon \tilde{X}] \\
&= E[\beta X \tilde{X}] + E[\epsilon] E[\tilde{X}]\\
&= \beta E[X \tilde{X}]\\
&= \beta E[X E(\tilde{X}|X)]\\
&= \beta E[X \rho X]\\
&= \beta \rho\\
\end{aligned}$

$\begin{aligned}
\begin{bmatrix}
y\\ X\\ \tilde{X}\\
\end{bmatrix} 
\sim N \left(
\begin{bmatrix} 0\\ 0\\ 0 \end{bmatrix}
,
\begin{bmatrix} 
\beta^2 + 1 & \beta & \beta\rho\\ 
\beta & 1 & \rho^2 \\ 
\beta\rho & \rho^2 & 1 \end{bmatrix} \right)
\end{aligned}$ 

$\begin{aligned}
R^2 
&= 1 - \frac{Var(y|\tilde{X})}{Var(y)} \\
&= 1- \frac{\beta^2 + 1 - \beta^2 \rho ^2}{\beta^2 + 1} \\
\end{aligned}$

**Scenario 2**: We only have 1 legacy trial with 2 correlated feature  $\tilde{X_1}, \tilde{X_2}$ both with correlation $\rho$ such that $\tilde{X_1}|X = x = \rho x + \sqrt{1-\rho^2} Z_1$, 
$\tilde{X_2}|X = x = \rho x + \sqrt{1-\rho^2} Z_2$

Oracle $R^2$ is calculated as follows:

$\begin{aligned}
cov(\tilde{X_1}, \tilde{X_2}) 
&= E[\tilde{X_1} \tilde{X_2}] - E[\tilde{X_1}]E[\tilde{X_2}] \\
&= E[\tilde{X_1} E[\tilde{X_2}| \tilde{X_1}] - 0 \\
&= E[\tilde{X_1} E[\rho x + \sqrt{1-\rho^2} Z_2| \tilde{X_1}]] \\
&= E[\tilde{X_1} E[\rho x | \tilde{X_1}]] \\
&= E[\tilde{X_1} \rho E[x | \tilde{X_1}]] \\
&= E[\tilde{X_1} \rho^2] \\
&= \rho ^ 2\\
\end{aligned}$


$\begin{aligned}
\begin{bmatrix}
y\\ \tilde{X}_1\\ \tilde{X}_2\\
\end{bmatrix} 
\sim N \left( \begin{bmatrix}  0\\ 0\\ 0 \end{bmatrix},
\begin{bmatrix} 
\beta^2 + 1 & \beta\rho & \beta\rho\\ 
\beta\rho & 1 & \rho^2\\ 
\beta\rho & \rho^2 & 1 \end{bmatrix} \right)
\end{aligned}$


### Causal feature not observed and trial unknown {.tabset .tabset-pills .tabset-fade}
**Scenario 1**: We have 2 legacy trials: legacy trial 1 has correlated feature $\tilde{X_1}$ and legacy trial 2 has correlated feature $\tilde{X_2}$ both with correlation $\rho$ such that

$$\begin{aligned}
\tilde{X_1}|X = x, t = 1 = \rho x + \sqrt{1-\rho^2} Z_1 \\
\tilde{X_2}|X = x, t = 2 = \rho x + \sqrt{1-\rho^2} Z_2 \\
\end{aligned}$$

The covariance matrix between $y, \tilde{X_1}, \tilde{X_2}$ is presented and oracle $R^2$ is calculated as follows.

$\begin{aligned}
cov(\tilde{X_1}, \tilde{X_2}) 
&= E[\tilde{X_1} \tilde{X_2}] - E[\tilde{X_1}] E[\tilde{X_2}]\\
&= E[\tilde{X_1} \tilde{X_2}|t = 1] Pr(t = 1) + E[\tilde{X_1} \tilde{X_2}|t = 2] Pr(t = 2)\\
&= \frac{1}{2} * 0 *  + \frac{1}{2} * 0\\
&= 0
\\
cov(X, \tilde{X_1}) 
&= E[X\tilde{X_1}] - E[X]E[\tilde{X_1}] \\
&= E[X\tilde{X_1}|t = 1] Pr(t = 1) + E[X\tilde{X_1}|t = 2] Pr(t = 2)\\
&= \frac{1}{2}E[X E(\tilde{X_1}|X)]+ \frac{1}{2} * 0\\
&= \frac{1}{2}E[X E(\rho X + \sqrt{1-\rho^2} Z_{1}|X)] \\
&= \frac{\rho}{2}\\
\\
cov(Y, \tilde{X_1}) 
&= E[Y\tilde{X_1}] - E[Y]E[\tilde{X_1}] \\
&= E[\tilde{X_1} E[Y|\tilde{X_1}]] \\
&= E[\tilde{X_1} E[\beta x + \epsilon|\tilde{X_1}]]\\
&= \beta E[\tilde{X_1} E[X|\tilde{X_1}]]\\
&= \frac{\beta \rho}{2}
\end{aligned}$



$\begin{aligned}
\begin{bmatrix}
y\\  \tilde{X}_1\\ \tilde{X}_2\\
\end{bmatrix} 
\sim N \left( \begin{bmatrix} 0\\ 0\\ 0 \end{bmatrix},
\begin{bmatrix} 
\beta^2 + 1  & \frac{\beta\rho}{2} & \frac{\beta\rho}{2}\\ 
\frac{\beta\rho}{2}  & 1 & 0\\ 
\frac{\beta\rho}{2} & 0 & 1 \end{bmatrix} \right)
\\
\end{aligned}$

**Scenario 2**: We have 3 legacy trials: legacy trial 1 has correlated feature $\tilde{X_1}, \tilde{X_2}$, and legacy trial 2 has correlated feature $\tilde{X_3}, \tilde{X_4}$ and legacy trial 3 has correlated features $\tilde{X_5}, \tilde{X_6}$ such that 

$$\begin{aligned}
\tilde{X_1}|X = x, t = 1 = \rho x + \sqrt{1-\rho^2} Z_1 \\
\tilde{X_2}|X = x, t = 1 = \rho x + \sqrt{1-\rho^2} Z_2 \\
\tilde{X_3}|X = x, t = 2 = \rho x + \sqrt{1-\rho^2} Z_3 \\
\tilde{X_4}|X = x, t = 2 = \rho x + \sqrt{1-\rho^2} Z_4 \\
\tilde{X_5}|X = x, t = 3 = \rho x + \sqrt{1-\rho^2} Z_5 \\
\tilde{X_6}|X = x, t = 3 = \rho x + \sqrt{1-\rho^2} Z_6 \\
\\
\end{aligned}$$

$\begin{aligned}
cov(\tilde{X_1}, \tilde{X_3}) 
&= E[\tilde{X_1} \tilde{X_3}] - E[\tilde{X_1}] E[\tilde{X_3}]\\
&= E[\tilde{X_1} \tilde{X_3}|t = 1] Pr(t = 1) + E[\tilde{X_1} \tilde{X_3}|t = 2] Pr(t = 2) + E[\tilde{X_1} \tilde{X_3}|t = 3] Pr(t = 3)\\
& = 0\\
\\
cov(X, \tilde{X_1}) 
&= E[X\tilde{X_1}] - E[X]E[\tilde{X_1}]\\
&= E[\tilde{X_1} \tilde{X}|t = 1] Pr(t = 1) + E[\tilde{X_1} \tilde{X}|t = 2] Pr(t = 2) + E[\tilde{X_1} \tilde{X}|t = 3] Pr(t = 3)\\
&= \frac{1}{3}E[X E(\tilde{X_1}|X)] + \frac{2}{3} * 0\\
&= \frac{1}{3}E[X E(\rho X + \sqrt{1-\rho^2} Z_{1}|X)]\\
&= \frac{\rho}{3}\\
\\
cov(\tilde{X_1}, \tilde{X_2}) 
&= E[\tilde{X_1} \tilde{X_2}] - E[\tilde{X_1}]E[\tilde{X_2}] \\
&= E[\tilde{X_1} \tilde{X_2}|t = 1] Pr(t = 1) + E[\tilde{X_1} \tilde{X_2}|t = 2] Pr(t = 2) +
E[\tilde{X_1} \tilde{X_2}|t = 3] Pr(t = 3)\\
&= \frac{1}{3} E[\tilde{X_1} \tilde{X_2}|t = 1] + \frac{2}{3} * 0\\
&= \frac{\rho ^ 2}{3}\\
\\
cov(Y, \tilde{X_1}) 
&= E[Y\tilde{X_1}] - E[Y]E[\tilde{X_1}] \\
&= E[\tilde{X_1} E[Y|\tilde{X_1}]] \\
&= E[\tilde{X_1} E[\beta x + \epsilon|\tilde{X_1}]]\\
&= \beta E[\tilde{X_1} E[X|\tilde{X_1}]]\\
&= \frac{\beta \rho}{3}\\
\end{aligned}$


$\begin{aligned}
\begin{bmatrix}
y\\ \tilde{X}_1\\ \tilde{X}_2 \\ \tilde{X}_3\\ \tilde{X}_4 \\ \tilde{X}_5\\ \tilde{X}_6\\
\end{bmatrix} 
\sim N \left( \begin{bmatrix} 0\\ 0\\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix} 
\beta^2 + 1 & \frac{\beta\rho}{3} & \frac{\beta\rho}{3} & \frac{\beta\rho}{3} & \frac{\beta\rho}{3} & \frac{\beta\rho}{3}& \frac{\beta\rho}{3}\\ 
\frac{\beta\rho}{3} & 1   & \frac{\rho^2}{3} & 0 & 0 & 0 & 0 \\ 
\frac{\beta\rho}{3} & \frac{\rho ^ 2}{3}   & 1 & 0 & 0 & 0 & 0 \\ 
\frac{\beta\rho}{3} & 0   & 0 & 1 & \frac{\rho^2}{3} & 0 & 0 \\ 
\frac{\beta\rho}{3} & 0   & 0 & \frac{\rho^2}{3} & 1 & 0 & 0 \\ 
\frac{\beta\rho}{3} & 0   & 0 & 0 & 0 & 1 & \frac{\rho^2}{3} \\ 
\frac{\beta\rho}{3} & 0   & 0 & 0 & 0 & \frac{\rho^2}{3} & 1\\ 
\end{bmatrix} \right)
\\
\end{aligned}$